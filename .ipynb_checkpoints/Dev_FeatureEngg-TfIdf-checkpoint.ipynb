{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Notebook/adviewRecomm/version3/temp/\n"
     ]
    }
   ],
   "source": [
    "## import data processing/cleaning , data modeling libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import re as re\n",
    "import datetime as datetime\n",
    "import numpy as np\n",
    "import collections\n",
    "import string\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "    \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#from gensim.models import doc2vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import fasttext\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "t0 = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1)\n",
      "                                                text\n",
      "0  bromwell high is a cartoon comedy . it ran at ...\n",
      "1  story of a man who has unnatural feelings for ...\n"
     ]
    }
   ],
   "source": [
    "outData43 = pd.read_csv(\"reviews.txt\",encoding=\"utf-8\",header=None,names=[\"text\"])\n",
    "print(outData43.shape)\n",
    "print(outData43.head(n=2)) # 39:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['khan',\n",
       " 'shah',\n",
       " 'marianne',\n",
       " 'song',\n",
       " 'chak',\n",
       " 'rukh',\n",
       " 'salim',\n",
       " 'cruz',\n",
       " 'sukhvinder',\n",
       " 'title',\n",
       " 'india',\n",
       " 'full',\n",
       " 'singh']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NLP PRE-PROCESSING\n",
    "\n",
    "#text=\" the JanaSena Party Formation Day celebrations || LIVE funny celebration|| Pawan Kalyan || Guntur\"\n",
    "text=\"Chak De India | Full Title Song | Shah Rukh Khan | Sukhvinder Singh | Salim | Marianne D'Cruz\"\n",
    "\n",
    "def nltk_clean_sent(line):\n",
    "    if len(line)>0:\n",
    "        ## remove the punctuation/emoticons/digits/multispaces with single from the line\n",
    "        ## dont make lowercase before the pos tagging\n",
    "        line_lower = line.strip()\n",
    "        line_punct = re.sub('['+string.punctuation+']',' ',line_lower)\n",
    "        line_emots = re.sub(r'[\\u200b-\\u2fff]+',' ',line_punct)\n",
    "        line_digis = re.sub(r'[0-9]+',' ',line_emots)\n",
    "        line_spaces = re.sub(r'[\\s]+',' ',line_digis)\n",
    "        line = line_spaces\n",
    "    return line\n",
    "\n",
    "def nltk_extract_postags(line):\n",
    "    cleaned_str = ''\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    tokens_pos = PerceptronTagger().tag(tokens)\n",
    "    #print(tokens_pos)\n",
    "    # noun tags\n",
    "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
    "    # adjectives\n",
    "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
    "    # verbs\n",
    "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
    "    \n",
    "    for tag_word in tokens_pos:\n",
    "        if tag_word[1] in nltk_tags:\n",
    "            if len(tag_word[0])>2:\n",
    "                cleaned_str += tag_word[0]+' '\n",
    "    return cleaned_str.strip().lower()\n",
    "\n",
    "def nltk_apply_lemma(line):\n",
    "    tokens_lemmas = [WordNetLemmatizer().lemmatize(word) for word in line.split()]\n",
    "    ## stemming\n",
    "    tokens_stops = [word for word in tokens_lemmas if word not in stopwords.words('english')]\n",
    "    tokens_stops = [word for word in tokens_stops if len(word.strip())>2]\n",
    "    tokens_stops = list(set(tokens_stops))\n",
    "    return tokens_stops\n",
    "\n",
    "def nltk_extract_tags(line):\n",
    "        ## tokenize the sentence/get tokens that contains only letters\n",
    "        line_clean = nltk_clean_sent(line)\n",
    "        ## apply postags to the words and get only couple of tags and word length >2\n",
    "        tokens_pos = nltk_extract_postags(line_clean)\n",
    "        ## apply lemmatize/stemming and remove stopwords\n",
    "        token_lemma = nltk_apply_lemma(tokens_pos)\n",
    "        return token_lemma\n",
    "    \n",
    "nltk_extract_tags(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF transformation\n",
    "def getTfIdfMetrics(outData44):\n",
    "    t0 = datetime.datetime.now()\n",
    "    tfidf_transform = TfidfVectorizer(tokenizer=nltk_extract_tags,min_df=3,max_df=0.95,stop_words='english',use_idf=True,ngram_range=(1,1))\n",
    "    #terms = tfidf_transform.get_feature_names()\n",
    "    #print(len(terms))\n",
    "    #print(terms[:100])\n",
    "    tfidf_vecto = tfidf_transform.fit_transform(outData44[\"text\"])\n",
    "    print(tfidf_vecto.shape)\n",
    "    # got shape of (83806, 149542) - without translation\n",
    "    t1 = datetime.datetime.now()\n",
    "    print(\"END TIME after TFIDF =\",t1)\n",
    "    print(\"time taken until TFIDF=\",(t1-t0))\n",
    "    return tfidf_vecto\n",
    "\n",
    "## PRINTING FIRST 100 FEATURES\n",
    "#getTfIdfMetrics(outData44)\n",
    "#terms = tfidf_transform.get_feature_names()\n",
    "#print(terms[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DOC2VEC model\n",
    "# #outData44_en_sample = outData44_en[50:55]\n",
    "\n",
    "# cleaneddoc = [nltk_extract_tags(text) for text in list(outData44_en[\"v_title_trans\"])]\n",
    "# taggeddoc = [doc2vec.TaggedDocument(val,[idx]) for idx,val in enumerate(cleaneddoc)]\n",
    "\n",
    "# d2vmodel = Doc2Vec(size=100,min_count=2,alpha=0.025,min_alpha=0.025)\n",
    "# d2vmodel.build_vocab(taggeddoc)\n",
    "# #print(d2vmodel[0])\n",
    "# d2vmodel.train(taggeddoc,total_examples=d2vmodel.corpus_count,epochs=10,start_alpha=0.002,end_alpha=-0.016)\n",
    "# d2vmodel_vecs = [d2vmodel.infer_vector(val) for idx,val in enumerate(cleaneddoc)]\n",
    "# #print(d2vmodel_vecs[0])\n",
    "\n",
    "##feature_vecto = d2vmodel.docvecs.doctag_syn0\n",
    "##print(len(d2vmodel_vecs))\n",
    "\n",
    "## WORD2VEC model # Word2Vec\n",
    "def getWord2VecMetrics(outData44):\n",
    "    t0 = datetime.datetime.now()\n",
    "    cleaneddoc = [nltk_extract_tags(text) for text in list(outData44[\"text\"])]\n",
    "    cleaneddoc = [text for text in cleaneddoc if len(text)>0]\n",
    "    w2vmodel = Word2Vec(cleaneddoc,min_count=2,size=300)\n",
    "    w2v_vecs = w2vmodel.wv.syn0\n",
    "    t1 = datetime.datetime.now()\n",
    "    print(\"END TIME after word2vec =\",t1)\n",
    "    print(\"time taken until word2vec=\",(t1-t0))\n",
    "    return w2v_vecs\n",
    "#feature_vecto = getWord2VecMetrics(outData44,lang)\n",
    "#print(feature_vecto.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL FUNCTION DEFINITION\n",
    "#feature_vecto = np.stack(d2vmodel.docvecs)\n",
    "#def getModelDataTfIdf(outData5,lang,feature_vecto):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END TIME after word2vec = 2018-07-07 23:23:40.780616\n",
      "time taken until word2vec= 0:23:51.622409\n",
      "(37007, 200)\n",
      "TIME taken for Modeling= 0:23:51.724596\n"
     ]
    }
   ],
   "source": [
    "startT = datetime.datetime.now()\n",
    "\n",
    "## feature engineering\n",
    "#feature_vecto = getTfIdfMetrics(outData43)\n",
    "feature_vecto = getWord2VecMetrics(outData43)\n",
    "print(feature_vecto.shape)\n",
    "## Model Data Function Call\n",
    "#clustData2 = getModelDataTfIdf(outData5,lang,feature_vecto)\n",
    "#print(clustData2.shape)\n",
    "endT = datetime.datetime.now()\n",
    "print(\"TIME taken for Modeling=\",(endT-startT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR : ValueError: max_df corresponds to < documents than min_df - min_df=6,max_df=0.9 - pass dataframe column when transforming tfidf\n",
    "\n",
    "# #min_count=2,size=200\n",
    "# END TIME after word2vec = 2018-07-07 23:23:40.780616\n",
    "# time taken until word2vec= 0:23:51.622409\n",
    "# (37007, 200)\n",
    "# TIME taken for Modeling= 0:23:51.724596\n",
    "\n",
    "#min_count=1,size=200\n",
    "# END TIME after word2vec = 2018-07-07 22:48:00.570493\n",
    "# time taken until word2vec= 0:24:20.255714\n",
    "# (63044, 200)\n",
    "# TIME taken for Modeling= 0:24:20.365022\n",
    "\n",
    "# min_df=3,max_df=0.95\n",
    "# (25000, 29447)\n",
    "# END TIME after TFIDF = 2018-07-07 22:08:19.848881\n",
    "# time taken until TFIDF= 0:22:22.619855\n",
    "# (25000, 29447)\n",
    "# TIME taken for Modeling= 0:22:22.635482\n",
    "        \n",
    "#     from sklearn.cluster import k_means_\n",
    "#     from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
